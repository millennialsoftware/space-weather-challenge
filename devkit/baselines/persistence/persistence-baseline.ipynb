{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NRLMSIS-Persistence Baseline\n",
    "\n",
    "This baseline is a very naive approach to the problem. Its main purpose is to guide participants through the different steps of preparing and testing a model, helping them understand the workflow required to make their own submissions.\n",
    "\n",
    "In time series problems, the **persistence baseline** is often used to assess model performance. This approach simply takes the last observed value of the input and propagates it through the output, replicating the initial value as predictions for all future steps. It can be described as follows:\n",
    "\n",
    "$$\\hat{y}_{t+k} = y_t$$\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "    &\\hat{y}_{t+k} &:& \\ \\text{Predicted value at future time } t+k, \\\\\n",
    "    &y_t           &:& \\ \\text{Observed value at the current time } t, \\\\\n",
    "    &k             &:& \\ \\text{Forecast horizon (number of steps into the future)}.\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "For example, if \\( y_t = 5 \\), then for any \\( k \\):\n",
    "\n",
    "\n",
    "$$\\hat{y}_{t+1} = \\hat{y}_{t+2} = \\hat{y}_{t+3} = \\dots = 5$$\n",
    "\n",
    "\n",
    "Although this approach may seem overly simplistic, it is surprisingly effective in certain cases, to the point that beating it can be challenging. This is because, in time series prediction tasks, the closer the future time steps are to the present, the more similar their values tend to be due to local temporal dependencies. As a result, when there are no significant outliers, this method can deliver reasonably good performance.\n",
    "\n",
    "In this particular case, as we need to propagate and do not know the values to propagate from the beginning, we will combine it with the NRLMSIS model to obtain predictions for the initial date along the propagated orbit at each time step. To achieve this, we will use the [`pymsis`](https://swxtrec.github.io/pymsis/) library. We will then propagate our results using the [custom propagator](https://github.com/ARCLab-MIT/STORM-AI-propagator/tree/main), which is provided as part of the development tools and makes use of the [`Orekit`](https://www.orekit.org/) library.\n",
    "\n",
    "> *Note*: The `#|export` tags are used to export the code block to a script file called `atm.py` using the [`nbdev`](https://nbdev.fast.ai/) CLI tool. This allows us to use these classes in other scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "mBS3_C9310j4"
   },
   "outputs": [],
   "source": [
    "# | default_exp atm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pymsis import msis\n",
    "import dill\n",
    "\n",
    "# Orekit imports\n",
    "import orekit\n",
    "from org.orekit.time import AbsoluteDate\n",
    "from org.orekit.utils import PVCoordinates, Constants\n",
    "from org.orekit.frames import Frame\n",
    "from org.orekit.models.earth.atmosphere import PythonAtmosphere\n",
    "from org.hipparchus.geometry.euclidean.threed import Vector3D\n",
    "from org.orekit.frames import FramesFactory, Frame\n",
    "from org.orekit.bodies import CelestialBodyFactory, OneAxisEllipsoid\n",
    "from org.orekit.utils import IERSConventions\n",
    "from org.orekit.time import TimeScalesFactory\n",
    "\n",
    "\n",
    "# Initialize Orekit JVM\n",
    "vm = orekit.initVM()\n",
    "from orekit.pyhelpers import setup_orekit_curdir\n",
    "setup_orekit_curdir()\n",
    "\n",
    "# Import the propagator script\n",
    "from propagator import prop_orbit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQSGLS311o02"
   },
   "source": [
    "Although the `pymsis` library does not require us to provide input data for the model, we will load the OMNI2 data available to participants as part of the challenge dataset. Here, we simply read the data file, retain the necessary columns, and perform basic preprocessing to fill missing values with the next available value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "PTvC2wg41i0J"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>ap_index_nT</th>\n",
       "      <th>f10.7_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-11-29 00:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-11-29 01:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-11-29 02:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-11-29 03:00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-11-29 04:00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>2014-01-27 20:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>139.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>2014-01-27 21:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>139.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>2014-01-27 22:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>139.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>2014-01-27 23:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>139.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>2014-01-28 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>152.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1441 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Timestamp  ap_index_nT  f10.7_index\n",
       "0    2013-11-29 00:00:00            4        125.0\n",
       "1    2013-11-29 01:00:00            4        125.0\n",
       "2    2013-11-29 02:00:00            4        125.0\n",
       "3    2013-11-29 03:00:00            9        125.0\n",
       "4    2013-11-29 04:00:00            9        125.0\n",
       "...                  ...          ...          ...\n",
       "1436 2014-01-27 20:00:00            3        139.6\n",
       "1437 2014-01-27 21:00:00            6        139.6\n",
       "1438 2014-01-27 22:00:00            6        139.6\n",
       "1439 2014-01-27 23:00:00            6        139.6\n",
       "1440 2014-01-28 00:00:00            0        152.5\n",
       "\n",
       "[1441 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load OMNI2 data directly\n",
    "file_path = './dataset/test/OMNI2/omni2-wu001.csv'\n",
    "omni2_data = pd.read_csv(file_path, usecols=['Timestamp', 'f10.7_index', 'ap_index_nT'])\n",
    "\n",
    "# Process OMNI2 data immediately after loading\n",
    "omni2_data['Timestamp'] = pd.to_datetime(omni2_data['Timestamp'])\n",
    "omni2_data = omni2_data.ffill()\n",
    "omni2_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-UkwQc32L-B"
   },
   "source": [
    "Now that we have the data, let's start creating the base class where we will make calls to the NRLMSIS model, always using the same date but varying the position. To do so, we will process some of the ap indices to be consumed by the `msis.run()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Ckf4OeoH2GYp"
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "class PersistenceMSIS():\n",
    "    def __init__(self, omni2_data):\n",
    "        \"\"\"\n",
    "        Initialize the Persistence Model.\n",
    "\n",
    "        Args:\n",
    "          - omni2_data (pd.DataFrame): OMNI2 data containing Ap, F10.7, and other parameters.\n",
    "\n",
    "        \"\"\"\n",
    "        self.initial_date = None\n",
    "        self.omni2_data = omni2_data\n",
    "\n",
    "    def run(self, dt, lon, lat, alt):\n",
    "        \"\"\"\n",
    "        Runs the MSIS model for the initial date using OMNI2 data to avoid online calls.\n",
    "\n",
    "        Parameters:\n",
    "            datetime_input (datetime): Datetime for the simulation.\n",
    "            lon (float): Longitude in degrees.\n",
    "            lat (float): Latitude in degrees.\n",
    "            alt (float): Altitude in km.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Output from the MSIS model.\n",
    "        \"\"\"\n",
    "        # We want to replicate the initial state through the output so we only\n",
    "        # keep the initial date\n",
    "        if self.initial_date is None:\n",
    "            self.initial_date = dt\n",
    "\n",
    "        # Find the closest row in OMNI2 data\n",
    "        row = self.omni2_data.loc[self.omni2_data['Timestamp'] == self.initial_date]\n",
    "        \n",
    "        f107_daily = row['f10.7_index'].values[0]\n",
    "        ap_current = row['ap_index_nT'].values[0]\n",
    "\n",
    "        # Prepare Ap indices using the helper function\n",
    "        aps = self._prepare_ap_indices(self.initial_date, ap_current)\n",
    "\n",
    "        # Run the MSIS model\n",
    "        result = msis.run(\n",
    "            dates=[self.initial_date],\n",
    "            lons=[lon],\n",
    "            lats=[lat],\n",
    "            alts=[alt],\n",
    "            f107s=[f107_daily],\n",
    "            aps=[aps]\n",
    "        )\n",
    "\n",
    "        return result[0,0]  # Return the density for the specific point\n",
    "\n",
    "    def _prepare_ap_indices(self, datetime_input, ap_current):\n",
    "        \"\"\"\n",
    "        Private helper function to compute Ap indices and averages required for MSIS.\n",
    "\n",
    "        Parameters:\n",
    "            datetime_input (datetime): Datetime for the simulation.\n",
    "            ap_current (float): Current daily Ap value.\n",
    "\n",
    "        Returns:\n",
    "            list: Prepared Ap array for MSIS input.\n",
    "        \"\"\"\n",
    "        index = self.omni2_data.index[self.omni2_data['Timestamp'] == datetime_input][0]\n",
    "\n",
    "        # Compute 3-hourly Ap indices\n",
    "        ap_3hr_indices = [\n",
    "            self.omni2_data.iloc[index - i]['ap_index_nT'] if (index - i) >= 0 else ap_current\n",
    "            for i in range(0, 4)\n",
    "        ]\n",
    "\n",
    "        # Compute averages for specific periods\n",
    "        ap_12_33_avg = np.mean([\n",
    "            self.omni2_data.iloc[index - i]['ap_index_nT'] if (index - i) >= 0 else ap_current\n",
    "            for i in range(12, 34, 3)\n",
    "        ])\n",
    "        ap_36_57_avg = np.mean([\n",
    "            self.omni2_data.iloc[index - i]['ap_index_nT'] if (index - i) >= 0 else ap_current\n",
    "            for i in range(36, 58, 3)\n",
    "        ])\n",
    "\n",
    "        # Prepare Ap array\n",
    "        aps = [\n",
    "            ap_current,  # Daily Ap\n",
    "            ap_3hr_indices[0],  # Current 3-hour Ap\n",
    "            ap_3hr_indices[1],  # 3 hours before\n",
    "            ap_3hr_indices[2],  # 6 hours before\n",
    "            ap_3hr_indices[3],  # 9 hours before\n",
    "            ap_12_33_avg,       # Average of 12-33 hours prior\n",
    "            ap_36_57_avg        # Average of 36-57 hours prior\n",
    "        ]\n",
    "\n",
    "        return aps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create the atmospheric model. To do so, we are using the boilerplate included as part of the propagator in the documentation. As you can see in the template, we only need to modify the instantiation and the `getDensity()` method. In this case, we will also retrieve the geodetic coordinates, as these are the ones the MSIS model can consume. Then, we simply call the `PersistenceMSIS.run()` method to execute the NRLMSIS model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "cY5KrACe2KsD"
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "class MSISPersistenceAtmosphere(PythonAtmosphere):\n",
    "    \"\"\"\n",
    "    CustomAtmosphere is a custom implementation of the PythonAtmosphere class\n",
    "    that uses the PersistenceModel to compute atmospheric density and velocity.\n",
    "\n",
    "    Attributes:\n",
    "        atm (PersistenceModel): An instance of the PersistenceModel.\n",
    "        earth (Body): The central body (Earth) for the atmospheric model.\n",
    "\n",
    "    Methods:\n",
    "        getMSISPersistence(input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "            Generates persistent MSIS data using the PersistenceModel.\n",
    "\n",
    "        getDensity(date: AbsoluteDate, position: Vector3D, frame: Frame) -> float:\n",
    "            Computes the atmospheric density at a given date, position, and frame\n",
    "            using the PersistenceModel output.\n",
    "\n",
    "        _position_to_geo(position: Vector3D) -> Tuple[float, float, float]:\n",
    "            Helper method to convert position to latitude, longitude, and altitude.\n",
    "    \"\"\"\n",
    "    def __init__(self, omni2, **kwargs):\n",
    "        super().__init__()\n",
    "        self.atm = PersistenceMSIS(omni2)\n",
    "\n",
    "        r_Earth = Constants.IERS2010_EARTH_EQUATORIAL_RADIUS #m\n",
    "        self.itrf = FramesFactory.getITRF(IERSConventions.IERS_2010, True) # International Terrestrial Reference Frame, earth fixed\n",
    "        self.earth = OneAxisEllipsoid(\n",
    "                         r_Earth,\n",
    "                         Constants.IERS2010_EARTH_FLATTENING,\n",
    "                         self.itrf\n",
    "                    )\n",
    "\n",
    "    def getDensity(self, date: AbsoluteDate, position: Vector3D, frame: Frame) -> float:\n",
    "        \"\"\"\n",
    "        Compute the atmospheric density at a given date, position, and frame using the PersistenceModel output.\n",
    "\n",
    "        Args:\n",
    "            date (AbsoluteDate): The date for which to compute density.\n",
    "            position (Vector3D): The position in the given frame.\n",
    "            frame (Frame): The reference frame.\n",
    "\n",
    "        Returns:\n",
    "            float: The computed atmospheric density.\n",
    "        \"\"\"\n",
    "        lat, lon, alt = self._position_to_geo(position, date)\n",
    "\n",
    "        # Convert date\n",
    "        time_str = date.toString(0)\n",
    "        dt = pd.to_datetime(time_str).tz_localize(None)\n",
    "\n",
    "        # Get persistence model output\n",
    "        density = self.atm.run(dt, lon, lat, alt)\n",
    "\n",
    "        return float(density)\n",
    "\n",
    "\n",
    "    def getVelocity(self, date: AbsoluteDate, position: Vector3D, frame: Frame):\n",
    "        '''\n",
    "        Get the inertial velocity of atmosphere molecules.\n",
    "        By default, atmosphere is supposed to have a null\n",
    "        velocity in the central body frame.</p>\n",
    "        '''\n",
    "        # get the transform from body frame to the inertial frame\n",
    "        bodyToFrame = self.earth.getBodyFrame().getKinematicTransformTo(frame, date)\n",
    "        # Inverse transform the position to the body frame\n",
    "        posInBody = bodyToFrame.getStaticInverse().transformPosition(position)\n",
    "        # Create PVCoordinates object assuming zero velocity in body frame\n",
    "        pv_body = PVCoordinates(posInBody, Vector3D.ZERO)\n",
    "        # Transform the position/velocity (PV) coordinates to the given frame\n",
    "        pvFrame = bodyToFrame.transformOnlyPV(pv_body)\n",
    "        # Return the velocity in the current frame\n",
    "        return pvFrame.getVelocity()\n",
    "\n",
    "    def _position_to_geo(self, positionICRF, date):\n",
    "            \"\"\"\n",
    "            Converts a position vector (in ICRF frame) to geodetic coordinates (lat, lon, alt).\n",
    "    \n",
    "            Parameters:\n",
    "            positionICRF: Vector3D, position vector in ICRF frame.\n",
    "            date: AbsoluteDate, the date of the position.\n",
    "    \n",
    "            Returns:\n",
    "            tuple: (latitude, longitude, altitude) in degrees and meters.\n",
    "            \"\"\"\n",
    "            # Create a PVCoordinates object (assuming zero velocity)\n",
    "            pvICRF = PVCoordinates(positionICRF, Vector3D.ZERO)\n",
    "    \n",
    "            # Transform position from ICRF to ECEF (ITRF)\n",
    "            transform = self.earth.getBodyFrame().getTransformTo(self.itrf, date)\n",
    "            pvECEF = transform.transformPVCoordinates(pvICRF)\n",
    "            positionECEF = pvECEF.getPosition()\n",
    "    \n",
    "            # Convert the ECEF position to geodetic coordinates\n",
    "            geodeticPoint = self.earth.transform(positionECEF, self.itrf, date)\n",
    "    \n",
    "            # Extract latitude, longitude, and altitude\n",
    "            latitude = geodeticPoint.getLatitude()  # radians\n",
    "            longitude = geodeticPoint.getLongitude()  # radians\n",
    "            altitude = geodeticPoint.getAltitude()  # meters\n",
    "    \n",
    "            # Convert radians to degrees for latitude and longitude\n",
    "            latitudeDeg = math.degrees(latitude)\n",
    "            longitudeDeg = math.degrees(longitude)\n",
    "    \n",
    "            return latitudeDeg, longitudeDeg, altitude\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have implemented our atmospheric model, let's execute it to check if it is working properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1129717726840402e-19"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atm = MSISPersistenceAtmosphere(omni2_data)\n",
    "date = AbsoluteDate(2013, 11, 29, 0, 0, 0.000, TimeScalesFactory.getUTC()\n",
    ")\n",
    "\n",
    "mu = Constants.IERS2010_EARTH_MU #m^3/s^2\n",
    "degree = 70\n",
    "torder = 70\n",
    "cr = 1.0\n",
    "utc = TimeScalesFactory.getUTC()\n",
    "sun = CelestialBodyFactory.getSun()\n",
    "\n",
    "# Initialize the Vector3D for position\n",
    "position = Vector3D(5_870_038.485921082, 2_396_433.1768343644, 2_396_433.176834364)\n",
    "\n",
    "# Initialize the Frame (EME2000)\n",
    "frame = FramesFactory.getEME2000() \n",
    "\n",
    "atm.getDensity(date, position, frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIlBcVAQ2aan"
   },
   "source": [
    "# Final Model\n",
    "\n",
    "Now that we have the atmospheric model, we simply need to apply the propagator to obtain predictions for the orbital density. This is what participants should do if they decide to approach the challenge by creating a model that generates density grids, but not the density at each point of the object's orbit.\n",
    "\n",
    "In this particular case, the `PersistenceModel()` class is created as a `pytorch` `nn.Module` to demonstrate the basic functioning of this library for creating custom models. Here, we run the propagator with our atmospheric model and then transform the results into a `DataFrame`. Usually, you would add other methods to design and train the model, but since our model does not require training, as it is a *wrapper* of another model, we will not implement that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "MjXK8KEL2ck-"
   },
   "outputs": [],
   "source": [
    "class PersistenceModel(nn.Module):\n",
    "    def __init__(self, plot_trajectory=False):\n",
    "        super().__init__()\n",
    "        self.plot = plot_trajectory\n",
    "    \n",
    "    def forward(self, omni2_data, initial_state={}):        \n",
    "        states, densities = prop_orbit(\n",
    "                                initial_state, \n",
    "                                MSISPersistenceAtmosphere,\n",
    "                                atm_model_data=omni2_data, \n",
    "                                plot_trajectory=self.plot\n",
    "                            )\n",
    "\n",
    "        return self._convert_to_df(states, densities)\n",
    "\n",
    "\n",
    "    def _convert_to_df(self, states, densities):\n",
    "        \"\"\"\n",
    "        Generates a DataFrame with timestamps and atmospheric densities.\n",
    "\n",
    "        Parameters:\n",
    "        - states (list of SpacecraftState): List of spacecraft states from the propagator.\n",
    "        - densities (list of float): List of atmospheric densities corresponding to each state.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: A DataFrame with columns ['timestamp', 'density'].\n",
    "        \"\"\"\n",
    "        # Initialize a list to hold data\n",
    "        density_data = []\n",
    "\n",
    "        # Iterate through states and densities\n",
    "        for state, density in zip(states, densities):\n",
    "            # Extract timestamp from the state\n",
    "            timestamp = pd.to_datetime(state.getDate().toString(0))  # Convert to pandas datetime\n",
    "\n",
    "            # Append data to the list\n",
    "            density_data.append({'Timestamp': timestamp, 'Density (kg/m3)': density})\n",
    "\n",
    "        # Convert the list to a DataFrame\n",
    "        df = pd.DataFrame(density_data)\n",
    "\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that is done, let's execute it and check if it works properly. We will also plot the trajectory to see how the propagator performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting propagation...:  11%|â–ˆ         | 46/431 [00:16<02:06,  3.05it/s]"
     ]
    }
   ],
   "source": [
    "model = PersistenceModel(plot_trajectory=True)\n",
    "predictions = model(omni2_data)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, we save the \"trained\" model to the folder `/trained_model/`, from where we can retrieve it in the future when we want to execute it during our submission. We use `dill` to properly save all the dependencies of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './trained_model/persistence_model.pkl', pickle_module=dill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Test\n",
    "\n",
    "Here, you can also test the `submission.py` script to ensure it is working correctly. This script will inform the execution node on our servers about the entry point of your submission, so you need to implement all the logic on how to run the submission inside it. Be very careful when checking that this file works properly, and also test it by executing the Docker container using the instructions found in this baseline folder.\n",
    "\n",
    "In this case, we will only execute it with a reduced amount of test files and initial states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 submission.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
